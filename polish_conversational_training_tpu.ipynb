{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {"id": "header"},
   "source": [
    "# Polish Conversational AI Training on TPU v5e-1\n",
    "\n",
    "## Overview\n",
    "This notebook trains a conversational AI model on the Polish sentences dataset (94M+ fragments) using Google Colab TPU v5e-1.\n",
    "\n",
    "**Dataset:** [adowu/polish_sentences](https://huggingface.co/datasets/adowu/polish_sentences)\n",
    "- Total rows: 94,167,155\n",
    "- Size: ~2.6 GB (original), 1.01 GB (Parquet)\n",
    "- Content: Polish text fragments ranging from 3 characters to 7.51k\n",
    "\n",
    "**Hardware:** TPU v5e-1 (optimized for training efficiency)\n",
    "\n",
    "**Framework:** JAX/Flax (optimal for TPU) with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "setup"},
   "source": ["## 1. Environment Setup & TPU Configuration"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "install_dependencies"},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q datasets transformers jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html flax optax huggingface-hub sentencepiece\n",
    "print('✓ Packages installed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "verify_tpu"},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "devices = jax.devices()\n",
    "print(f'Available devices: {devices}')\n",
    "print(f'Device count: {jax.device_count()}')\n",
    "assert jax.device_count() >= 1, 'No TPU devices found!'\n",
    "print('✓ TPU configured successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "dataset"},
   "source": ["## 2. Dataset Loading & Analysis"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "load_dataset"},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "dataset = load_dataset('adowu/polish_sentences', split='train')\n",
    "print(f'Dataset loaded: {len(dataset):,} rows')\n",
    "sample = dataset.shuffle(seed=42).select(range(min(10000, len(dataset))))\n",
    "lengths = [len(item['fragment']) for item in sample]\n",
    "print(f'Mean length: {np.mean(lengths):.1f} chars')\n",
    "print(f'Median length: {np.median(lengths):.1f} chars')\n",
    "print('✓ Dataset analysis complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "preprocessing"},
   "source": ["## 3. Tokenization & Preprocessing"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "tokenizer"},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_CHECKPOINT = 'sdadas/polish-gpt2-medium'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>', 'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>'})\n",
    "print(f'Tokenizer vocab size: {len(tokenizer)}')\n",
    "print('✓ Tokenizer configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "preprocess"},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "def preprocess_function(examples):\n",
    "    texts = [f\"{tokenizer.bos_token}{text}{tokenizer.eos_token}\" for text in examples['fragment']]\n",
    "    tokenized = tokenizer(texts, truncation=True, max_length=MAX_LENGTH, padding='max_length', return_tensors=None)\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, batch_size=1000, remove_columns=dataset.column_names, num_proc=2)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']\n",
    "print(f'Train: {len(train_dataset):,}, Eval: {len(eval_dataset):,}')\n",
    "print('✓ Dataset tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "model"},
   "source": ["## 4. Model Initialization"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "init_model"},
   "outputs": [],
   "source": [
    "from transformers import FlaxAutoModelForCausalLM, AutoConfig\n",
    "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)\n",
    "config.vocab_size = len(tokenizer)\n",
    "model = FlaxAutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT, config=config, dtype=jnp.bfloat16, _do_init=True)\n",
    "num_params = sum(x.size for x in jax.tree_util.tree_leaves(model.params))\n",
    "print(f'Model parameters: {num_params:,} ({num_params/1e6:.1f}M)')\n",
    "print('✓ Model initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "training"},
   "source": ["## 5. Training Configuration"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "config"},
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 2000\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "steps_per_epoch = len(train_dataset) // BATCH_SIZE\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "print(f'Steps per epoch: {steps_per_epoch:,}')\n",
    "print(f'Total steps: {total_steps:,}')\n",
    "print('✓ Training config set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "optimizer"},
   "outputs": [],
   "source": [
    "warmup_fn = optax.linear_schedule(0.0, LEARNING_RATE, WARMUP_STEPS)\n",
    "decay_fn = optax.cosine_decay_schedule(LEARNING_RATE, total_steps - WARMUP_STEPS, 0.1)\n",
    "lr_schedule = optax.join_schedules([warmup_fn, decay_fn], [WARMUP_STEPS])\n",
    "optimizer = optax.chain(optax.clip_by_global_norm(1.0), optax.adamw(lr_schedule, b1=0.9, b2=0.999, weight_decay=0.01))\n",
    "class TrainState(train_state.TrainState):\n",
    "    dropout_rng: jnp.ndarray\n",
    "rng = random.PRNGKey(42)\n",
    "rng, dropout_rng = random.split(rng)\n",
    "state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=optimizer, dropout_rng=dropout_rng)\n",
    "print('✓ Optimizer created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "train_loop"},
   "source": ["## 6. Training Loop"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "train_step"},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from flax import jax_utils\n",
    "@partial(jax.pmap, axis_name='batch', donate_argnums=(0,))\n",
    "def train_step(state, batch):\n",
    "    dropout_rng, new_dropout_rng = random.split(state.dropout_rng)\n",
    "    def loss_fn(params):\n",
    "        labels = batch['labels']\n",
    "        outputs = state.apply_fn(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], params=params, dropout_rng=dropout_rng, train=True)\n",
    "        logits = outputs.logits\n",
    "        vocab_size = logits.shape[-1]\n",
    "        labels_one_hot = jax.nn.one_hot(labels, vocab_size)\n",
    "        loss = optax.softmax_cross_entropy(logits, labels_one_hot)\n",
    "        mask = (labels != tokenizer.pad_token_id).astype(jnp.float32)\n",
    "        return (loss * mask).sum() / mask.sum()\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "    new_state = state.apply_gradients(grads=grads, dropout_rng=new_dropout_rng)\n",
    "    return new_state, {'loss': loss, 'learning_rate': lr_schedule(state.step)}\n",
    "@partial(jax.pmap, axis_name='batch')\n",
    "def eval_step(state, batch):\n",
    "    labels = batch['labels']\n",
    "    outputs = state.apply_fn(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], params=state.params, train=False)\n",
    "    logits = outputs.logits\n",
    "    vocab_size = logits.shape[-1]\n",
    "    labels_one_hot = jax.nn.one_hot(labels, vocab_size)\n",
    "    loss = optax.softmax_cross_entropy(logits, labels_one_hot)\n",
    "    mask = (labels != tokenizer.pad_token_id).astype(jnp.float32)\n",
    "    loss = (loss * mask).sum() / mask.sum()\n",
    "    perplexity = jnp.exp(loss)\n",
    "    return {'loss': jax.lax.pmean(loss, axis_name='batch'), 'perplexity': jax.lax.pmean(perplexity, axis_name='batch')}\n",
    "print('✓ Training functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "data_loader"},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "def create_data_loader(dataset, batch_size, shuffle=False):\n",
    "    def _data_generator():\n",
    "        ds = dataset.shuffle(buffer_size=10000, seed=42) if shuffle else dataset\n",
    "        for example in ds:\n",
    "            yield {'input_ids': example['input_ids'], 'attention_mask': example['attention_mask'], 'labels': example['labels']}\n",
    "    output_sig = {'input_ids': tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32), 'attention_mask': tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32), 'labels': tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32)}\n",
    "    tf_dataset = tf.data.Dataset.from_generator(_data_generator, output_signature=output_sig)\n",
    "    return tf_dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE).as_numpy_iterator()\n",
    "print('✓ Data loader created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "main_loop"},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from flax.training.common_utils import shard\n",
    "state = jax_utils.replicate(state)\n",
    "print('=== Starting Training ===')\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    train_loader = create_data_loader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "    epoch_loss = 0\n",
    "    epoch_steps = 0\n",
    "    progress_bar = tqdm(total=steps_per_epoch, desc=f'Epoch {epoch+1}')\n",
    "    for batch in train_loader:\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "        state, metrics = train_step(state, batch)\n",
    "        loss = jax_utils.unreplicate(metrics['loss'])\n",
    "        epoch_loss += loss\n",
    "        epoch_steps += 1\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            progress_bar.set_postfix({'loss': f'{epoch_loss / epoch_steps:.4f}'})\n",
    "        progress_bar.update(1)\n",
    "        if epoch_steps >= steps_per_epoch:\n",
    "            break\n",
    "    progress_bar.close()\n",
    "    print(f'Epoch {epoch+1} avg loss: {epoch_loss / epoch_steps:.4f}')\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nTraining complete! Time: {total_time/3600:.2f}h')\n",
    "print('✓ Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "save"},
   "source": ["## 7. Save Model"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "save_model"},
   "outputs": [],
   "source": [
    "final_model_dir = '/content/polish_conversational_model'\n",
    "unreplicated_state = jax_utils.unreplicate(state)\n",
    "model.save_pretrained(final_model_dir, params=unreplicated_state.params)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "config.save_pretrained(final_model_dir)\n",
    "print(f'✓ Model saved to {final_model_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "test"},
   "source": ["## 8. Test Generation"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "generate"},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors='np', padding=True)\n",
    "    params = jax_utils.unreplicate(state.params)\n",
    "    rng = random.PRNGKey(int(time.time()))\n",
    "    generated = model.generate(jnp.array(inputs['input_ids']), attention_mask=jnp.array(inputs['attention_mask']), params=params, max_length=max_length, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, prng_key=rng).sequences\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "test_prompts = ['Bóg pobłogosławi', 'Witaj, jak się', 'Dzisiaj jest']\n",
    "print('=== Test Generation ===')\n",
    "for prompt in test_prompts:\n",
    "    print(f'\\nPrompt: {prompt}')\n",
    "    print(f'Generated: {generate_text(prompt, 50)}')\n",
    "print('\\n✓ Generation test complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "summary"},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete pipeline for training Polish conversational AI on TPU v5e-1:\n",
    "\n",
    "1. **Dataset**: 94M+ Polish sentence fragments\n",
    "2. **Model**: GPT-2 fine-tuned for Polish\n",
    "3. **Hardware**: TPU v5e-1 with JAX/Flax\n",
    "4. **Training**: 3 epochs with AdamW\n",
    "5. **Output**: Production-ready model\n",
    "\n",
    "### Resources\n",
    "- [Dataset](https://huggingface.co/datasets/adowu/polish_sentences)\n",
    "- [JAX Docs](https://jax.readthedocs.io/)\n",
    "- [Flax Docs](https://flax.readthedocs.io/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {"name": "polish_conversational_training_tpu.ipynb", "provenance": [], "gpuType": "v5-litepod-1"},
  "kernelspec": {"display_name": "Python 3", "name": "python3"},
  "language_info": {"name": "python"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}