{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Polish Conversational AI Training on TPU v5e-1\n",
    "\n",
    "## Overview\n",
    "This notebook trains a conversational AI model on the Polish sentences dataset (94M+ fragments) using Google Colab TPU v5e-1.\n",
    "\n",
    "**Dataset:** [adowu/polish_sentences](https://huggingface.co/datasets/adowu/polish_sentences)\n",
    "- Total rows: 94,167,155\n",
    "- Size: ~2.6 GB (original), 1.01 GB (Parquet)\n",
    "- Content: Polish text fragments ranging from 3 characters to 7.51k\n",
    "\n",
    "**Hardware:** TPU v5e-1 (optimized for training efficiency)\n",
    "\n",
    "**Framework:** JAX/Flax (optimal for TPU) with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup & TPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages for TPU v5e-1\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q \\\n",
    "    datasets==2.19.0 \\\n",
    "    transformers==4.40.0 \\\n",
    "    jax[tpu]==0.4.26 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html \\\n",
    "    flax==0.8.3 \\\n",
    "    optax==0.2.2 \\\n",
    "    huggingface-hub==0.23.0 \\\n",
    "    sentencepiece==0.2.0 \\\n",
    "    wandb==0.17.0 \\\n",
    "    tensorboard==2.16.2\n",
    "\n",
    "print(\"✓ Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_tpu"
   },
   "outputs": [],
   "source": [
    "# Verify TPU availability and configuration\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import os\n",
    "\n",
    "# Check TPU devices\n",
    "devices = jax.devices()\n",
    "print(f\"Available devices: {devices}\")\n",
    "print(f\"Device count: {jax.device_count()}\")\n",
    "print(f\"Local device count: {jax.local_device_count()}\")\n",
    "print(f\"Process count: {jax.process_count()}\")\n",
    "\n",
    "# TPU v5e-1 should show 1 device\n",
    "assert jax.device_count() >= 1, \"No TPU devices found!\"\n",
    "print(f\"\\n✓ TPU v5e-1 configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_analysis"
   },
   "source": [
    "## 2. Dataset Analysis & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load Polish sentences dataset\n",
    "print(\"Loading dataset from HuggingFace...\")\n",
    "dataset = load_dataset(\"adowu/polish_sentences\", split=\"train\", streaming=False)\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} rows\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print(f\"\\nFirst 5 examples:\")\n",
    "for i in range(min(5, len(dataset))):\n",
    "    print(f\"{i+1}. {dataset[i]['fragment']}\")\n",
    "\n",
    "print(\"\\n✓ Dataset loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_dataset"
   },
   "outputs": [],
   "source": [
    "# Comprehensive dataset analysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample analysis (use subset for speed)\n",
    "sample_size = min(100000, len(dataset))\n",
    "sample = dataset.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "# Text length statistics\n",
    "lengths = [len(item['fragment']) for item in sample]\n",
    "\n",
    "print(\"=== Dataset Statistics ===")\n",
    "print(f\"Total samples: {len(dataset):,}\")\n",
    "print(f\"Analyzed sample: {sample_size:,}\")\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Mean: {np.mean(lengths):.2f} characters\")\n",
    "print(f\"  Median: {np.median(lengths):.2f} characters\")\n",
    "print(f\"  Min: {np.min(lengths)} characters\")\n",
    "print(f\"  Max: {np.max(lengths)} characters\")\n",
    "print(f\"  Std Dev: {np.std(lengths):.2f}\")\n",
    "\n",
    "# Percentile analysis\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "print(\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(lengths, p)\n",
    "    print(f\"  {p}th: {val:.0f} characters\")\n",
    "\n",
    "# Vocabulary sample\n",
    "all_text = ' '.join([item['fragment'] for item in sample[:1000]])\n",
    "words = all_text.split()\n",
    "unique_words = len(set(words))\n",
    "print(f\"\\nVocabulary (from 1k samples): ~{unique_words:,} unique words\")\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(lengths, bins=50, edgecolor='black')\n",
    "plt.xlabel('Fragment Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Fragment Lengths')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist([l for l in lengths if l < 200], bins=50, edgecolor='black')\n",
    "plt.xlabel('Fragment Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution (< 200 chars)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## 3. Data Preprocessing & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_tokenizer"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use Polish-optimized tokenizer\n",
    "# Options: 'allegro/herbert-base-cased', 'sdadas/polish-gpt2-medium', 'google/mt5-small'\n",
    "MODEL_CHECKPOINT = \"sdadas/polish-gpt2-medium\"  # Best for Polish conversational\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_CHECKPOINT}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Add special tokens for conversation\n",
    "special_tokens = {\n",
    "    'pad_token': '<|pad|>',\n",
    "    'bos_token': '<|startoftext|>',\n",
    "    'eos_token': '<|endoftext|>'\n",
    "}\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    num_added = tokenizer.add_special_tokens(special_tokens)\n",
    "    print(f\"Added {num_added} special tokens\")\n",
    "\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"\\n✓ Tokenizer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_dataset"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_LENGTH = 512  # Maximum sequence length\n",
    "BATCH_SIZE = 32  # Per-device batch size\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize and prepare examples for training.\"\"\"\n",
    "    # Add BOS and EOS tokens\n",
    "    texts = [\n",
    "        f\"{tokenizer.bos_token}{text}{tokenizer.eos_token}\"\n",
    "        for text in examples['fragment']\n",
    "    ]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Process dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=4,  # Parallel processing\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenized dataset: {len(tokenized_dataset)} samples\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")\n",
    "print(\"\\n✓ Dataset tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_dataset"
   },
   "outputs": [],
   "source": [
    "# Split into train/validation\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(eval_dataset):,}\")\n",
    "print(\"\\n✓ Dataset split complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## 4. Model Architecture & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_model"
   },
   "outputs": [],
   "source": [
    "from transformers import FlaxAutoModelForCausalLM, AutoConfig\n",
    "import flax.linen as nn\n",
    "from typing import Optional\n",
    "\n",
    "# Model configuration\n",
    "print(\"Loading base model configuration...\")\n",
    "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Resize embeddings for new special tokens\n",
    "config.vocab_size = len(tokenizer)\n",
    "\n",
    "# Initialize model with Flax (optimized for TPU)\n",
    "print(\"Initializing Flax model...\")\n",
    "model = FlaxAutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    config=config,\n",
    "    dtype=jnp.bfloat16,  # Use bfloat16 for TPU efficiency\n",
    "    _do_init=True\n",
    ")\n",
    "\n",
    "# Resize token embeddings\n",
    "if len(tokenizer) != config.vocab_size:\n",
    "    # This is handled by config.vocab_size above\n",
    "    pass\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Type: {config.model_type}\")\n",
    "print(f\"  Layers: {config.n_layer if hasattr(config, 'n_layer') else config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {config.n_embd if hasattr(config, 'n_embd') else config.hidden_size}\")\n",
    "print(f\"  Attention heads: {config.n_head if hasattr(config, 'n_head') else config.num_attention_heads}\")\n",
    "print(f\"  Vocabulary: {len(tokenizer)}\")\n",
    "print(f\"  Max position: {config.n_positions if hasattr(config, 'n_positions') else config.max_position_embeddings}\")\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(x.size for x in jax.tree_util.tree_leaves(model.params))\n",
    "print(f\"\\nTotal parameters: {num_params:,} ({num_params/1e6:.1f}M)\")\n",
    "print(\"\\n✓ Model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_setup"
   },
   "source": [
    "## 5. Training Configuration & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "from flax.training.common_utils import shard\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 2000\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 3\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LOGGING_STEPS = 100\n",
    "EVAL_STEPS = 1000\n",
    "SAVE_STEPS = 5000\n",
    "\n",
    "# Calculate training steps\n",
    "steps_per_epoch = len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "\n",
    "print(\"=== Training Configuration ===")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch:,}\")\n",
    "print(f\"Total training steps: {total_steps:,}\")\n",
    "print(\"\\n✓ Configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_optimizer"
   },
   "outputs": [],
   "source": [
    "# Create learning rate schedule\n",
    "def create_learning_rate_fn(\n",
    "    train_steps: int,\n",
    "    warmup_steps: int,\n",
    "    learning_rate: float\n",
    ") -> optax.Schedule:\n",
    "    \"\"\"Create learning rate schedule with warmup and cosine decay.\"\"\"\n",
    "    warmup_fn = optax.linear_schedule(\n",
    "        init_value=0.0,\n",
    "        end_value=learning_rate,\n",
    "        transition_steps=warmup_steps\n",
    "    )\n",
    "    decay_fn = optax.cosine_decay_schedule(\n",
    "        init_value=learning_rate,\n",
    "        decay_steps=train_steps - warmup_steps,\n",
    "        alpha=0.1\n",
    "    )\n",
    "    schedule_fn = optax.join_schedules(\n",
    "        schedules=[warmup_fn, decay_fn],\n",
    "        boundaries=[warmup_steps]\n",
    "    )\n",
    "    return schedule_fn\n",
    "\n",
    "# Create optimizer\n",
    "lr_schedule = create_learning_rate_fn(total_steps, WARMUP_STEPS, LEARNING_RATE)\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Gradient clipping\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=0.9,\n",
    "        b2=0.999,\n",
    "        eps=1e-8,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create train state\n",
    "class TrainState(train_state.TrainState):\n",
    "    dropout_rng: jnp.ndarray\n",
    "\n",
    "rng = random.PRNGKey(42)\n",
    "rng, dropout_rng = random.split(rng)\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.__call__,\n",
    "    params=model.params,\n",
    "    tx=optimizer,\n",
    "    dropout_rng=dropout_rng\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizer and training state created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_loop"
   },
   "source": [
    "## 6. Training Loop Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_step"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from flax import jax_utils\n",
    "\n",
    "# Define training step\n",
    "@partial(jax.pmap, axis_name='batch', donate_argnums=(0,))\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Execute one training step.\"\"\"\n",
    "    dropout_rng, new_dropout_rng = random.split(state.dropout_rng)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        labels = batch.pop('labels')\n",
    "        outputs = state.apply_fn(\n",
    "            **batch,\n",
    "            params=params,\n",
    "            dropout_rng=dropout_rng,\n",
    "            train=True\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        vocab_size = logits.shape[-1]\n",
    "        labels_one_hot = jax.nn.one_hot(labels, vocab_size)\n",
    "        loss = optax.softmax_cross_entropy(logits, labels_one_hot)\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        mask = (labels != tokenizer.pad_token_id).astype(jnp.float32)\n",
    "        loss = (loss * mask).sum() / mask.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    \n",
    "    # Average gradients across devices\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "    \n",
    "    new_state = state.apply_gradients(grads=grads, dropout_rng=new_dropout_rng)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'learning_rate': lr_schedule(state.step)\n",
    "    }\n",
    "    \n",
    "    return new_state, metrics\n",
    "\n",
    "# Define evaluation step\n",
    "@partial(jax.pmap, axis_name='batch')\n",
    "def eval_step(state, batch):\n",
    "    \"\"\"Execute one evaluation step.\"\"\"\n",
    "    labels = batch.pop('labels')\n",
    "    outputs = state.apply_fn(\n",
    "        **batch,\n",
    "        params=state.params,\n",
    "        train=False\n",
    "    )\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Calculate loss\n",
    "    vocab_size = logits.shape[-1]\n",
    "    labels_one_hot = jax.nn.one_hot(labels, vocab_size)\n",
    "    loss = optax.softmax_cross_entropy(logits, labels_one_hot)\n",
    "    \n",
    "    # Mask padding tokens\n",
    "    mask = (labels != tokenizer.pad_token_id).astype(jnp.float32)\n",
    "    loss = (loss * mask).sum() / mask.sum()\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = jnp.exp(loss)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': jax.lax.pmean(loss, axis_name='batch'),\n",
    "        'perplexity': jax.lax.pmean(perplexity, axis_name='batch')\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"✓ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loader"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "def create_data_loader(dataset, batch_size, shuffle=False):\n",
    "    \"\"\"Create efficient data loader for TPU.\"\"\"\n",
    "    def _data_generator():\n",
    "        if shuffle:\n",
    "            dataset_shuffled = dataset.shuffle(buffer_size=10000, seed=42)\n",
    "        else:\n",
    "            dataset_shuffled = dataset\n",
    "        \n",
    "        for example in dataset_shuffled:\n",
    "            yield {\n",
    "                'input_ids': example['input_ids'],\n",
    "                'attention_mask': example['attention_mask'],\n",
    "                'labels': example['labels']\n",
    "            }\n",
    "    \n",
    "    output_signature = {\n",
    "        'input_ids': tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32),\n",
    "        'attention_mask': tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32),\n",
    "        'labels': tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
    "    }\n",
    "    \n",
    "    tf_dataset = tf.data.Dataset.from_generator(\n",
    "        _data_generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    tf_dataset = tf_dataset.batch(batch_size, drop_remainder=True)\n",
    "    tf_dataset = tf_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return tf_dataset.as_numpy_iterator()\n",
    "\n",
    "print(\"✓ Data loader function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_training_loop"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# Initialize tracking\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'eval_loss': [],\n",
    "    'eval_perplexity': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "# Replicate state across TPU devices\n",
    "state = jax_utils.replicate(state)\n",
    "\n",
    "print(\"=== Starting Training ===")\n",
    "print(f\"Training on TPU v5e-1 with {jax.device_count()} device(s)\\n\")\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loader = create_data_loader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_steps = 0\n",
    "    \n",
    "    progress_bar = tqdm(total=steps_per_epoch, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        # Prepare batch for TPU\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "        \n",
    "        # Training step\n",
    "        state, metrics = train_step(state, batch)\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        loss = jax_utils.unreplicate(metrics['loss'])\n",
    "        lr = jax_utils.unreplicate(metrics['learning_rate'])\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_steps += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        # Logging\n",
    "        if global_step % LOGGING_STEPS == 0:\n",
    "            avg_loss = epoch_loss / epoch_steps\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            \n",
    "            training_history['train_loss'].append(float(avg_loss))\n",
    "            training_history['learning_rate'].append(float(lr))\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{avg_loss:.4f}\",\n",
    "                'lr': f\"{lr:.2e}\",\n",
    "                'steps/s': f\"{steps_per_sec:.2f}\"\n",
    "            })\n",
    "        \n",
    "        # Evaluation\n",
    "        if global_step % EVAL_STEPS == 0:\n",
    "            print(\"\\n\\nRunning evaluation...\")\n",
    "            eval_loader = create_data_loader(eval_dataset, BATCH_SIZE, shuffle=False)\n",
    "            \n",
    "            eval_metrics = []\n",
    "            for eval_batch in eval_loader:\n",
    "                eval_batch = {k: jnp.array(v) for k, v in eval_batch.items()}\n",
    "                eval_batch = shard(eval_batch)\n",
    "                \n",
    "                metrics = eval_step(state, eval_batch)\n",
    "                eval_metrics.append(jax_utils.unreplicate(metrics))\n",
    "            \n",
    "            eval_loss = jnp.mean(jnp.array([m['loss'] for m in eval_metrics]))\n",
    "            eval_perplexity = jnp.mean(jnp.array([m['perplexity'] for m in eval_metrics]))\n",
    "            \n",
    "            training_history['eval_loss'].append(float(eval_loss))\n",
    "            training_history['eval_perplexity'].append(float(eval_perplexity))\n",
    "            \n",
    "            print(f\"Eval Loss: {eval_loss:.4f}\")\n",
    "            print(f\"Eval Perplexity: {eval_perplexity:.4f}\\n\")\n",
    "        \n",
    "        # Checkpoint\n",
    "        if global_step % SAVE_STEPS == 0:\n",
    "            checkpoint_dir = f\"/content/checkpoints/step_{global_step}\"\n",
    "            print(f\"\\nSaving checkpoint to {checkpoint_dir}...\")\n",
    "            \n",
    "            # Unreplicate state\n",
    "            unreplicated_state = jax_utils.unreplicate(state)\n",
    "            \n",
    "            # Save model\n",
    "            model.save_pretrained(\n",
    "                checkpoint_dir,\n",
    "                params=unreplicated_state.params\n",
    "            )\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "            \n",
    "            print(\"Checkpoint saved!\\n\")\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if epoch_steps >= steps_per_epoch:\n",
    "            break\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # End of epoch summary\n",
    "    avg_epoch_loss = epoch_loss / epoch_steps\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"  Steps: {epoch_steps:,}\")\n",
    "\n",
    "# Training complete\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Total steps: {global_step:,}\")\n",
    "print(f\"Average steps/sec: {global_step/total_time:.2f}\")\n",
    "\n",
    "# Save training history\n",
    "with open('/content/training_history.json', 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Training history saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model"
   },
   "source": [
    "## 7. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_final_model"
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = \"/content/polish_conversational_model_final\"\n",
    "print(f\"Saving final model to {final_model_dir}...\")\n",
    "\n",
    "# Unreplicate state\n",
    "unreplicated_state = jax_utils.unreplicate(state)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\n",
    "    final_model_dir,\n",
    "    params=unreplicated_state.params\n",
    ")\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "# Save config\n",
    "config.save_pretrained(final_model_dir)\n",
    "\n",
    "print(\"\\n✓ Final model saved successfully\")\n",
    "print(f\"Model location: {final_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 8. Model Evaluation & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_generation"
   },
   "outputs": [],
   "source": [
    "# Test text generation\n",
    "def generate_text(prompt, max_length=100, temperature=0.8, top_p=0.9):\n",
    "    \"\"\"Generate text from prompt.\"\"\"\n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    input_ids = jnp.array(inputs['input_ids'])\n",
    "    attention_mask = jnp.array(inputs['attention_mask'])\n",
    "    \n",
    "    # Generate\n",
    "    rng = random.PRNGKey(int(time.time()))\n",
    "    \n",
    "    # Get unreplicated params for generation\n",
    "    params = jax_utils.unreplicate(state.params)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        params=params,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        prng_key=rng\n",
    "    ).sequences\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(\n",
    "        generated_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Bóg pobłogosławi\",\n",
    "    \"Witaj, jak się\",\n",
    "    \"Dzisiaj jest piękny\",\n",
    "    \"Polska to\",\n",
    "    \"Kocham\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing Model Generation ===")\n",
    "print()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    generated = generate_text(prompt, max_length=50)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 60)\n",
    "    print()\n",
    "\n",
    "print(\"✓ Generation test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## 9. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_metrics"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training loss\n",
    "axes[0, 0].plot(training_history['train_loss'])\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].set_xlabel('Logging Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Evaluation loss\n",
    "axes[0, 1].plot(training_history['eval_loss'], color='orange')\n",
    "axes[0, 1].set_title('Evaluation Loss')\n",
    "axes[0, 1].set_xlabel('Evaluation Step')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Perplexity\n",
    "axes[1, 0].plot(training_history['eval_perplexity'], color='green')\n",
    "axes[1, 0].set_title('Evaluation Perplexity')\n",
    "axes[1, 0].set_xlabel('Evaluation Step')\n",
    "axes[1, 0].set_ylabel('Perplexity')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(training_history['learning_rate'], color='red')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].set_xlabel('Logging Step')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Metrics plotted and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export"
   },
   "source": [
    "## 10. Export & Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_to_hub"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo, upload_folder\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to HuggingFace (you'll need to provide your token)\n",
    "# Get token from: https://huggingface.co/settings/tokens\n",
    "\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Replace with your token\n",
    "MODEL_NAME = \"polish-conversational-gpt2-94m\"\n",
    "\n",
    "# Uncomment to upload\n",
    "# login(token=HF_TOKEN)\n",
    "# \n",
    "# # Create repository\n",
    "# repo_id = f\"YOUR_USERNAME/{MODEL_NAME}\"\n",
    "# create_repo(repo_id, exist_ok=True, private=False)\n",
    "# \n",
    "# # Upload model\n",
    "# upload_folder(\n",
    "#     folder_path=final_model_dir,\n",
    "#     repo_id=repo_id,\n",
    "#     commit_message=\"Upload trained Polish conversational model\"\n",
    "# )\n",
    "# \n",
    "# print(f\"✓ Model uploaded to https://huggingface.co/{repo_id}\")\n",
    "\n",
    "print(\"To upload, uncomment the code above and add your HF token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# Download model to local machine\n",
    "import shutil\n",
    "\n",
    "# Create zip archive\n",
    "print(\"Creating model archive...\")\n",
    "shutil.make_archive(\n",
    "    '/content/polish_conversational_model',\n",
    "    'zip',\n",
    "    final_model_dir\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model archived to: /content/polish_conversational_model.zip\")\n",
    "print(\"\\nDownload files from Colab:\")\n",
    "print(\"  - polish_conversational_model.zip\")\n",
    "print(\"  - training_history.json\")\n",
    "print(\"  - training_metrics.png\")\n",
    "\n",
    "# Uncomment to auto-download\n",
    "# from google.colab import files\n",
    "# files.download('/content/polish_conversational_model.zip')\n",
    "# files.download('/content/training_history.json')\n",
    "# files.download('/content/training_metrics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage"
   },
   "source": [
    "## 11. Model Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usage_example"
   },
   "outputs": [],
   "source": [
    "# Load trained model for inference\n",
    "from transformers import FlaxAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "loaded_model = FlaxAutoModelForCausalLM.from_pretrained(final_model_dir)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(final_model_dir)\n",
    "\n",
    "# Interactive generation\n",
    "def chat(prompt, max_length=150):\n",
    "    \"\"\"Generate conversational response.\"\"\"\n",
    "    inputs = loaded_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    outputs = loaded_model.generate(\n",
    "        jnp.array(inputs['input_ids']),\n",
    "        attention_mask=jnp.array(inputs['attention_mask']),\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=loaded_tokenizer.pad_token_id,\n",
    "        eos_token_id=loaded_tokenizer.eos_token_id\n",
    "    ).sequences\n",
    "    \n",
    "    response = loaded_tokenizer.decode(\n",
    "        outputs[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "print(\"=== Polish Conversational AI ===")\n",
    "print(\"\\nType 'quit' to exit\\n\")\n",
    "\n",
    "# Interactive loop (uncomment for interactive mode)\n",
    "# while True:\n",
    "#     user_input = input(\"You: \")\n",
    "#     if user_input.lower() == 'quit':\n",
    "#         break\n",
    "#     \n",
    "#     response = chat(user_input)\n",
    "#     print(f\"AI: {response}\\n\")\n",
    "\n",
    "# Demo examples\n",
    "demo_prompts = [\n",
    "    \"Cześć! Jak się masz?\",\n",
    "    \"Opowiedz mi o Polsce\",\n",
    "    \"Co sądzisz o\"\n",
    "]\n",
    "\n",
    "print(\"Demo conversations:\")\n",
    "for prompt in demo_prompts:\n",
    "    print(f\"\\nYou: {prompt}\")\n",
    "    response = chat(prompt)\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "print(\"\\n✓ Model ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete pipeline for training a Polish conversational AI model on Google Colab TPU v5e-1:\n",
    "\n",
    "1. **Dataset**: 94M+ Polish sentence fragments\n",
    "2. **Model**: GPT-2 architecture fine-tuned for Polish\n",
    "3. **Hardware**: TPU v5e-1 with JAX/Flax optimization\n",
    "4. **Training**: 3 epochs with AdamW optimizer\n",
    "5. **Output**: Production-ready conversational model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different hyperparameters\n",
    "- Train for more epochs for better convergence\n",
    "- Try different model architectures (T5, mT5)\n",
    "- Fine-tune on specific conversation domains\n",
    "- Deploy to production environment\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Dataset](https://huggingface.co/datasets/adowu/polish_sentences)\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)\n",
    "- [Flax Documentation](https://flax.readthedocs.io/)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [TPU Best Practices](https://cloud.google.com/tpu/docs/performance-guide)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "polish_conversational_training_tpu.ipynb",
   "provenance": [],
   "toc_visible": true,
   "gpuType": "v5-litepod-1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}